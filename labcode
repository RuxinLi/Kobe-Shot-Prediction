**Support Vector Classifier** Lab: 9.6.1
----------------------------------------

### Library

We will use the `e1071` library in R to demonstrate the support vector
classifier and the support vector machine.

-   Another option is the `LiblineaR` library
    -   which is useful for very large linear problems.

The `svm()` function can be used to fit a support vector classifier:

-   `y ~ .` uses the syntax for x and y similar to `lm()`
-   `data =` argument to pass through data
-   `kernel = 'linear'` denotes a linear decision boundary
-   `cost` argument allows us to specify the cost of a violation to the
    margin.
    -   when cost is small, margins will be wide, and many support
        vectors will be on the margin or will violate the margin.
    -   when cost argument is large, margins will be narrow and there
        will be a few support vectors on the margin or violating the
        margin.
-   `scale =` TRUE/FALSE to scale the data being passed through

### Data

We will demonstrate the use of this function on a two-dimensional
example so that we can plot the resulting decision boundary.

We begin by generating synthetic observations, which belong to two
classes, and checking whether the classes are linearly separable.

``` r
set.seed(1)
x <- matrix(rnorm(20*2), ncol = 2)
head(x,2)
```

    ##            [,1]      [,2]
    ## [1,] -0.6264538 0.9189774
    ## [2,]  0.1836433 0.7821363

``` r
tail(x,2)
```

    ##            [,1]      [,2]
    ## [19,] 0.8212212 1.1000254
    ## [20,] 0.5939013 0.7631757

``` r
y <- c(rep(-1,10), rep(1,10))
(y)
```

    ##  [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1

The statement below adds a value of one to both columns of the x matrix
between row 11 and 20

``` r
x[y==1,] <- x[y==1,] + 1
(x)
```

    ##             [,1]        [,2]
    ##  [1,] -0.6264538  0.91897737
    ##  [2,]  0.1836433  0.78213630
    ##  [3,] -0.8356286  0.07456498
    ##  [4,]  1.5952808 -1.98935170
    ##  [5,]  0.3295078  0.61982575
    ##  [6,] -0.8204684 -0.05612874
    ##  [7,]  0.4874291 -0.15579551
    ##  [8,]  0.7383247 -1.47075238
    ##  [9,]  0.5757814 -0.47815006
    ## [10,] -0.3053884  0.41794156
    ## [11,]  2.5117812  2.35867955
    ## [12,]  1.3898432  0.89721227
    ## [13,]  0.3787594  1.38767161
    ## [14,] -1.2146999  0.94619496
    ## [15,]  2.1249309 -0.37705956
    ## [16,]  0.9550664  0.58500544
    ## [17,]  0.9838097  0.60571005
    ## [18,]  1.9438362  0.94068660
    ## [19,]  1.8212212  2.10002537
    ## [20,]  1.5939013  1.76317575

``` r
plot(x, col = (2-y), pch = 19)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-5-1.png" width="75%" style="display: block; margin: auto;" />
The classes are not linearly separable.

### Fitting The Support Vector Classifier

Note, in order for the `svm()` function to perform classification, we
must encode the response as a **factor variable**.

``` r
dat <- data.frame(x = x, y = as.factor(y))
```

Now we call the svm() function with appropriate arguments.

``` r
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 10, scale = FALSE)
```

### Support Vector Classifier Output

Plot call takes two arguments. The first is the model output (`svmfit`)
and the second is the data (`dat`).

``` r
plot(svmfit, dat)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-8-1.png" width="75%" style="display: block; margin: auto;" />

Things to note for the classification plot

-   +1 feature space is shown in dark shade
-   -1 feature space is shown in light shade
-   decision boundary is linear although it appears jagged (due to the
    way the plotting function is implemented in the library)
-   axis have switched from the first plot of data
-   support vectors are plotted as X and the remaining observations are
    plotted as 0

From the above plot, we can identify 7 support vectors. We can determine
their identities with the index call.

``` r
svmfit$index
```

    ## [1]  1  2  5  7 14 16 17

The `summary()` command gives us some basic information.

``` r
summary(svmfit)
```

    ## 
    ## Call:
    ## svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10, 
    ##     scale = FALSE)
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  linear 
    ##        cost:  10 
    ## 
    ## Number of Support Vectors:  7
    ## 
    ##  ( 4 3 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  -1 1

This verifies our linear kernel and cost of 10. It also shows the number
of support vectors used (7) and to which class they belong (4 to one
class and 3 to the other).

### Updating The Cost Value

How does our model change if we use a smaller value for cost?

``` r
svmfit <- svm(y ~., data = dat, kernel = 'linear', cost = 0.1, scale = FALSE)
plot(svmfit, dat)
```

![](Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-11-1.png)

``` r
svmfit$index
```

    ##  [1]  1  2  3  4  5  7  9 10 12 13 14 15 16 17 18 20

As mentioned previously, a smaller value for cost means a wider margin
and more support vectors.

**Key Take-Away** The fact that only support vectors affect the
classifier is in line with the assertion that `cost` controls the
bias-variance trade-off.

-   When `cost` is large:
    -   margin is narrow
    -   few observations violate the margin, so there are less support
        vectors
    -   low bias
    -   high variance
-   When `cost` is small:
    -   margin is wide
    -   many observations violate the margin, so there are more support
        vectors
    -   low variance
    -   high bias

### Finding The Best Parameters

`e1071` has a built-in function `tune()` that performs 10-fold
cross-validation on a set of models in consideration. 10-Fold is the
default and may be changed.

The following command indicates that we want to compare SVMs with a
linear kernel, using a range of values of the `cost` parameter.

``` r
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = 'linear', 
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
```

The summary command allows us to access the cross-validation errors for
each of these models.

``` r
summary(tune.out)
```

    ## 
    ## Parameter tuning of 'svm':
    ## 
    ## - sampling method: 10-fold cross validation 
    ## 
    ## - best parameters:
    ##  cost
    ##   0.1
    ## 
    ## - best performance: 0.05 
    ## 
    ## - Detailed performance results:
    ##    cost error dispersion
    ## 1 1e-03  0.55  0.4377975
    ## 2 1e-02  0.55  0.4377975
    ## 3 1e-01  0.05  0.1581139
    ## 4 1e+00  0.15  0.2415229
    ## 5 5e+00  0.15  0.2415229
    ## 6 1e+01  0.15  0.2415229
    ## 7 1e+02  0.15  0.2415229

We see that the model reports cost, error, and dispersion for each
model.

Of particular significance, the model reports the best parameter (0.1)
and best error for that model (0.05).

The `tune()` function also stores the best model and can be accessed.

``` r
bestmod <- tune.out$best.model
summary(bestmod)
```

    ## 
    ## Call:
    ## best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, 
    ##     0.01, 0.1, 1, 5, 10, 100)), kernel = "linear")
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  linear 
    ##        cost:  0.1 
    ## 
    ## Number of Support Vectors:  16
    ## 
    ##  ( 8 8 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  -1 1

### SVM Predictions

We can use the `predict()` function to predict the class label on a set
of test observations.

First, let’s generate test set data.

``` r
xtest <- matrix(rnorm(20*2), ncol = 2)
ytest <- sample(c(-1,1), 20, rep = TRUE)
xtest[ytest == 1,] <- xtest[ytest == 1,] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
```

Now we make predictions using the test data.

``` r
ypred <- predict(bestmod, testdat)
mytable <- table(actual = testdat$y, predict = ypred)
(mytable)
```

    ##       predict
    ## actual -1 1
    ##     -1  9 2
    ##     1   1 8

With the best model cost value of 0.1 we correctly predicted 17 samples.
While misclassing 3 samples.

How would we have performed on a suboptimal cost metric? Let’s predict
once more with a cost value of 0.01.

``` r
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 0.01, scale = FALSE)
ypred <- predict(svmfit, testdat)
mytable2 <- table(actual = testdat$y, predict = ypred)
(mytable2)
```

    ##       predict
    ## actual -1  1
    ##     -1 11  0
    ##     1   6  3

Here we see how using a worse cost function has led to 3 additional
misclassifications.

### Exploring When Linear Separation Is True

Let’s explore when we do in fact have linear separation. Adjusting the
simulated data we can create this separation.

``` r
x[y==1,] <- x[y==1,] + 0.5
plot(x, col = (y+5)/2, pch = 19)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-18-1.png" width="75%" style="display: block; margin: auto;" />
We have created separation although very slightly and with a very narrow
window.

Now we will fit this new separated data with a very large `cost` so that
no observations are misclassified.

``` r
dat = data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 1e5)
summary(svmfit)
```

    ## 
    ## Call:
    ## svm(formula = y ~ ., data = dat, kernel = "linear", cost = 1e+05)
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  linear 
    ##        cost:  1e+05 
    ## 
    ## Number of Support Vectors:  3
    ## 
    ##  ( 1 2 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  -1 1

``` r
plot(svmfit, dat)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-19-1.png" width="75%" style="display: block; margin: auto;" />
As we know with large cost values, margins will be narrow and support
vectors will be few and very close to the margins.

This is evident here, using three support vectors we have a narrow
margin. Observations that are not support vectors are close to the
margin and this would be evidence that the model would not perform well
on test data. (Overfitting)

Now let’s try a smaller value of cost.

``` r
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 1)
summary(svmfit)
```

    ## 
    ## Call:
    ## svm(formula = y ~ ., data = dat, kernel = "linear", cost = 1)
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  linear 
    ##        cost:  1 
    ## 
    ## Number of Support Vectors:  7
    ## 
    ##  ( 4 3 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  -1 1

``` r
plot(svmfit, dat)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-20-1.png" width="75%" style="display: block; margin: auto;" />
With this new cost value of 1, we have in fact made one training error
however, we also obtained a much wider margin and made use of seven
support vectors.

This model with a smaller cost value would undoubtably, perform better
on unseen test data than the model with cost equal to 100,000.

**Support Vector Machine** Lab: 9.6.2
-------------------------------------

### Library

In order to fit an SVM using a non-linear kernel, we once again use the
`svm()` function but make changes to the `kernel =` argument.

-   `kernel = 'polynomial'` for polynomial boundary
    -   `degree =` to specify degree of polynomial
-   `kernel = 'radial'` for a radial boundary
    -   `gamma =` to specify gamma value for radial

### Data

Similar to before but with a few modifications, lets generate some data
that has a distinct non-linear class boundary.

``` r
set.seed(1)
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2
x[101:150,] <- x[101:150,] -2
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x = x, y = as.factor(y))
head(dat)
```

    ##        x.1        x.2 y
    ## 1 1.373546  2.4094018 1
    ## 2 2.183643  3.6888733 1
    ## 3 1.164371  3.5865884 1
    ## 4 3.595281  1.6690922 1
    ## 5 2.329508 -0.2852355 1
    ## 6 1.179532  4.4976616 1

The plot below shows that we in fact have a non-linear class boundary.

``` r
plot(x, col = y, pch = 19)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-22-1.png" width="75%" style="display: block; margin: auto;" />

### Fitting The Support Vector Classifier

First, the data is split into test and train samples.

``` r
train <- sample(200, 100)
```

Next, we fit the training data using the `svm()` function with a radial
kernel and gamma equal to 1.

``` r
svmfit <- svm(y ~ ., data = dat[train,], kernel = 'radial', gamma = 1, cost = 1)
plot(svmfit, dat[train,])
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-24-1.png" width="75%" style="display: block; margin: auto;" />
The plot shows the SVM has a decidedly non-linear boundary.

Using the `summary()` function we can garner some information about this
model.

``` r
summary(svmfit)
```

    ## 
    ## Call:
    ## svm(formula = y ~ ., data = dat[train, ], kernel = "radial", 
    ##     gamma = 1, cost = 1)
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  radial 
    ##        cost:  1 
    ## 
    ## Number of Support Vectors:  31
    ## 
    ##  ( 16 15 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  1 2

The plot above shows a fair number of misclassifications in the training
data. Increasing the cost function will reduce the training errors
however, this comes at a price of a more irregular decision boundary
with an increased risk of overfitting.

``` r
svmfit <- svm(y ~ ., data = dat[train,], kernel = 'radial', gamma = 1, cost = 1e5)
plot(svmfit, dat[train,])
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-26-1.png" width="75%" style="display: block; margin: auto;" />
Our decision boundary is now much more flexible but may come at a risk
when evaluated with the test data.

### Finding The Best Parameters

We can use the same `tune()` function as before to find the best value
for `cost` and for `gamma`.

``` r
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train,], kernel ='radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
```

    ## 
    ## Parameter tuning of 'svm':
    ## 
    ## - sampling method: 10-fold cross validation 
    ## 
    ## - best parameters:
    ##  cost gamma
    ##     1   0.5
    ## 
    ## - best performance: 0.07 
    ## 
    ## - Detailed performance results:
    ##     cost gamma error dispersion
    ## 1  1e-01   0.5  0.26 0.15776213
    ## 2  1e+00   0.5  0.07 0.08232726
    ## 3  1e+01   0.5  0.07 0.08232726
    ## 4  1e+02   0.5  0.14 0.15055453
    ## 5  1e+03   0.5  0.11 0.07378648
    ## 6  1e-01   1.0  0.22 0.16193277
    ## 7  1e+00   1.0  0.07 0.08232726
    ## 8  1e+01   1.0  0.09 0.07378648
    ## 9  1e+02   1.0  0.12 0.12292726
    ## 10 1e+03   1.0  0.11 0.11005049
    ## 11 1e-01   2.0  0.27 0.15670212
    ## 12 1e+00   2.0  0.07 0.08232726
    ## 13 1e+01   2.0  0.11 0.07378648
    ## 14 1e+02   2.0  0.12 0.13165612
    ## 15 1e+03   2.0  0.16 0.13498971
    ## 16 1e-01   3.0  0.27 0.15670212
    ## 17 1e+00   3.0  0.07 0.08232726
    ## 18 1e+01   3.0  0.08 0.07888106
    ## 19 1e+02   3.0  0.13 0.14181365
    ## 20 1e+03   3.0  0.15 0.13540064
    ## 21 1e-01   4.0  0.27 0.15670212
    ## 22 1e+00   4.0  0.07 0.08232726
    ## 23 1e+01   4.0  0.09 0.07378648
    ## 24 1e+02   4.0  0.13 0.14181365
    ## 25 1e+03   4.0  0.15 0.13540064

Here we see 25 models that were used for the five variations of cost and
five variations of gamma. Recorded are the best values for cost equal to
1 and best value of gamma equal to 0.5. This resulted in the best error
performance of 0.07.

### Best Model Predictions

Since, `tune()` stores our best model attained over the 10-fold
cross-validation, we can use that model to predict on our test data.

``` r
svmfitBest <- tune.out$best.model
mytable3 <- table(actual = dat[-train,'y'], pred = predict(svmfitBest,newdata = dat[-train,]))
(mytable3)
```

    ##       pred
    ## actual  1  2
    ##      1 67 10
    ##      2  2 21

Of the 100 test samples, 88 were correctly classified or 0.88 percent.
We misclassed 12 samples or 0.12 percent.

### Plot Alternatives

The lab in the book uses the plot function in the `e1071` library. This
plot is fairly crude, and can be difficult to understand. The author’s
wedsite has an alternative that is more understandable.

``` r
make.grid <- function(x, n=75){
  grange <- apply(x,2,range)
  x.1 <- seq(from = grange[1,1], to = grange[2,1], length = n)
  x.2 <- seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(x.1 = x.1, x.2 = x.2)
}
xgrid <- make.grid(x[train,])
ygrid <- predict(svmfitBest, xgrid)
plot(xgrid, col= c('red','blue')[as.numeric(ygrid)], pch = 20, cex = .2)
points(x[-train,], col = y[-train], pch = 19)
```

<img src="Lab-SVM-V2_files/figure-markdown_github/unnamed-chunk-29-1.png" width="75%" style="display: block; margin: auto;" />

``` r
#points(x[-train,][svmfitBest$index,], pch = 5, cex = 2)
```

The boundary can be viewed as the red/blue dots from the training data.
The two classes for the test data can be viewed as the large red/black
dots, and the support vector’s used will be circled in black diamonds.
Here we can see our 12 misclasses for the test data.
